{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss funciton are the method of evaluating how well the model that we have built works. It measures how far the prediction and actual target is . It is also called error or cost function. Our task in training AI models is to minimize this loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.L1Loss\n",
    "\n",
    "Measures the mean absolute error(MAE) . Given a target $`Y`$ and out predicted output $`(\\hat Y)`$, it calculates the mean absolute error between each data in these vector.\n",
    "\n",
    "Mean Absolute Error (MAE) = $``Y` -  \\hat Y  `$.\n",
    "\n",
    "```python\n",
    "torch.nn.L1Loss(  reduction = 'none   )\n",
    "```\n",
    "\n",
    "Parameters :\n",
    "* reduction(str) : Specifies the reduction to apply to the output: \n",
    "    'none' : no reduction will be applied\n",
    "    'mean' : the sum of the output will be divided by the number of elements in the output\n",
    "    'sum' : the output will be summed\n",
    "\n",
    "with `reduction` set to `none` nn.L1Loss is \n",
    "\n",
    "```math\n",
    "l(x,y) = L = {l_1,l_2,.......,l_N}^T , l_n = | x_n - y_n |\n",
    "\\text{where N is the total batch size.}\n",
    "\\\\ then \\\\\n",
    "\\begin{cases}\n",
    "mean(L) & \\text{if reduction = 'mean'} \\\\\n",
    "sum(L) & \\text{if reduction = 'sum'}\n",
    "\\end{cases}\n",
    "```\n",
    "\n",
    "`x` and `y` are tensors of arbitrary shapes with a total of n elements each.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(size = (5,3))\n",
    "y = torch.randn(size = (5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1955, -0.2542,  0.2666],\n",
      "        [-3.3301,  1.4459, -0.7097],\n",
      "        [-0.9593,  1.4890, -1.1341],\n",
      "        [ 3.4275, -1.4349,  1.0722],\n",
      "        [ 0.6549, -0.6395, -0.6933]])\n"
     ]
    }
   ],
   "source": [
    "Error = x - y\n",
    "print(Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we take L1 loss the errors absolute is only returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1955, 0.2542, 0.2666],\n",
      "        [3.3301, 1.4459, 0.7097],\n",
      "        [0.9593, 1.4890, 1.1341],\n",
      "        [3.4275, 1.4349, 1.0722],\n",
      "        [0.6549, 0.6395, 0.6933]])\n"
     ]
    }
   ],
   "source": [
    "L1_loss = nn.L1Loss( reduction = 'none' )\n",
    "loss = L1_loss(x,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the output when `reduction` is set to `mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2471)\n"
     ]
    }
   ],
   "source": [
    "print(nn.L1Loss(reduction= 'mean') (x,y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.MSELoss\n",
    "\n",
    "Measures the mean squared error (MSE). Given a target $`Y`$ and our predicted output $`(\\hat{Y})`$, it calculates the mean squared error between each element in these vectors.\n",
    "\n",
    "Mean Squared Error (MSE) = $`(Y - \\hat{Y})^2`$.\n",
    "\n",
    "```python\n",
    "torch.nn.MSELoss(reduction='none')\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "* `reduction` (str): Specifies the reduction to apply to the output:\n",
    "  * `'none'`: No reduction will be applied.\n",
    "  * `'mean'`: The sum of the output will be divided by the number of elements in the output.\n",
    "  * `'sum'`: The output will be summed.\n",
    "\n",
    "With `reduction` set to `none`, `nn.MSELoss` is:\n",
    "\n",
    "```math\n",
    "l(x, y) = L = \\{l_1, l_2, \\ldots, l_N\\}^T, \\quad l_n = (x_n - y_n)^2\n",
    "\\text{where N is the total batch size.}\n",
    "\\\\\n",
    "\\text{then} \\\\\n",
    "\\begin{cases}\n",
    "\\text{mean}(L) & \\text{if reduction = 'mean'} \\\\\n",
    "\\text{sum}(L) & \\text{if reduction = 'sum'}\n",
    "\\end{cases}\n",
    "```\n",
    "\n",
    "`x` and `y` are tensors of arbitrary shapes with a total of `N` elements each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss with no reduction:\n",
      " tensor([[ 1.4291,  0.0646,  0.0711],\n",
      "        [11.0898,  2.0907,  0.5037],\n",
      "        [ 0.9202,  2.2171,  1.2862],\n",
      "        [11.7475,  2.0588,  1.1497],\n",
      "        [ 0.4289,  0.4090,  0.4806]])\n"
     ]
    }
   ],
   "source": [
    "MSE_loss = nn.MSELoss(reduction='none')\n",
    "loss = MSE_loss(x, y)\n",
    "print(\"MSE Loss with no reduction:\\n\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss with mean reduction:\n",
      " tensor(2.3965)\n"
     ]
    }
   ],
   "source": [
    "mean_loss = nn.MSELoss(reduction='mean')(x, y)\n",
    "print(\"MSE Loss with mean reduction:\\n\", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss with sum reduction:\n",
      " tensor(35.9472)\n"
     ]
    }
   ],
   "source": [
    "sum_loss = nn.MSELoss(reduction='sum')(x, y)\n",
    "print(\"MSE Loss with sum reduction:\\n\", sum_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.BCELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Measures the Binary Cross Entropy (BCE) between the target and the predicted output. This loss is used for binary classification problems.\n",
    "\n",
    "Binary Cross Entropy Loss (BCE) is defined as:\n",
    "\n",
    "```math\n",
    "\\text{BCE}(x, y) = -w_n \\left( y \\cdot \\log(x) + (1 - y) \\cdot \\log(1 - x) \\right)\n",
    "```\n",
    "where $`x`$ is the predicted output and $`y`$ is the target.  \n",
    "$`w_n`$ refers to the weights of the class which does manual rescaling in case the two classes are unbalanced.\n",
    "\n",
    "```python\n",
    "torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "* `weight` (Tensor, optional): A manual rescaling weight given to each batch element. If given, it has to be a Tensor of size `nbatch`.\n",
    "* `reduction` (str): Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.\n",
    "\n",
    "With `reduction` set to `none`, `nn.BCELoss` is:\n",
    "\n",
    " ```math\n",
    "l(x, y) = L = \\{l_1, l_2, \\ldots, l_N\\}^T, \\quad l_n = -w_n \\left( y_n \\cdot \\log(x_n) + (1 - y_n) \\cdot \\log(1 - x_n) \\right)\n",
    "\\text{where N is the batch size.}\n",
    "\\\\\n",
    "\\text{then} \\\\\n",
    "\\begin{cases}\n",
    "\\text{mean}(L) & \\text{if reduction = 'mean'} \\\\\n",
    "\\text{sum}(L) & \\text{if reduction = 'sum'}\n",
    "\\end{cases}\n",
    " ```\n",
    "\n",
    "`x` and `y` are tensors of the same shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets create a random predicted outputs and target values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5892],\n",
       "         [0.4290],\n",
       "         [0.4028],\n",
       "         [0.5604],\n",
       "         [0.2005]]),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(seed = 100)\n",
    "x = torch.sigmoid(torch.randn(size=(5, 1)))  # Predicted outputs in range [0, 1]\n",
    "y = torch.randint(0, 2, size=(5, 1)).float()  # Binary target values\n",
    "x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In above case, we have a batch where there are 5 prediction made.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing BCELoss with no reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8896],\n",
      "        [0.5604],\n",
      "        [0.5155],\n",
      "        [0.8220],\n",
      "        [1.6071]])\n"
     ]
    }
   ],
   "source": [
    "bce_loss = nn.BCELoss(reduction='none')\n",
    "print(bce_loss(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets confirm it through our own formula**\n",
    "for a sample :   \n",
    "prdicted = 0.2005 , target = 1\n",
    " ```math\n",
    "\\text{BCE}(x, y) = -\\left( y \\cdot \\log(x) + (1 - y) \\cdot \\log(1 - x) \\right)\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.606941032235513"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_loss = - (1 * np.log(0.2005)  + (1 - 1) * np.log(1 - 0.2005) )\n",
    "calc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, it matches. Now similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE Loss with mean reduction:\n",
      " tensor(0.8789)\n",
      "BCE Loss with sum reduction:\n",
      " tensor(4.3946)\n"
     ]
    }
   ],
   "source": [
    "# Calculating the BCE loss with mean reduction\n",
    "mean_loss = nn.BCELoss(reduction='mean')(x, y)\n",
    "print(\"BCE Loss with mean reduction:\\n\", mean_loss)\n",
    "\n",
    "# Calculating the BCE loss with sum reduction\n",
    "sum_loss = nn.BCELoss(reduction='sum')(x, y)\n",
    "print(\"BCE Loss with sum reduction:\\n\", sum_loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.CrossEntropyLoss\n",
    "\n",
    "In the previous `BinaryCrossEntropy` loss, we calculated loss in case where there is only two classes. So for a more general case we use `CrossEntropyLoss`.\n",
    "Measures the Cross Entropy Loss between the input logits and target values. It is mostly used in classification problem. This loss combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.\n",
    "\n",
    "Given a target $`Y`$ (which is a class index) and our predicted output $`X`$ (which are raw, unnormalized logits of each class),X will have value for each class. It should be a tensor of size `C` for unbatched input. For batched 1 dimensional input the predicted output X is `(minibatch,C)` and if higherdimensional it is `(minibatch,C,d1,d2,....,dk)` like for 2 dimensional image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `reduction` set to `none`, `nn.CrossEntropyLoss` is:\n",
    "\n",
    " ```math\n",
    "l(x, y) = L = \\{l_1, l_2, \\ldots, l_N\\}^T, \\quad l_n = - w_{y_n} \\log\\left(\\frac{\\exp(x_{n,y_n})}{\\sum_{j=1}^{C} \\exp(x_{n,j})}\\right)\n",
    "\\text{where N is the batch size.}\n",
    "\\\\\n",
    "\\text{then} \\\\\n",
    "\\begin{cases}\n",
    "\\text{mean}(L) & \\text{if reduction = 'mean'} \\\\\n",
    "\\text{sum}(L) & \\text{if reduction = 'sum'}\n",
    "\\end{cases}\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, we have N batch prediction made and each prediction inside the batch has logit value for each class. $`w_{y_n}`$ refers to the weight assined to the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " it calculates the loss as follows:\n",
    " ```math\n",
    "\\text{Cross Entropy Loss} = -\\sum_{i=1}^{C} y_i \\log\\left(\\frac{\\exp(x_i)}{\\sum_{j=1}^{C} \\exp(x_j)}\\right)\n",
    " ```\n",
    "where `C` is the number of classes, $`y_i`$ is the ground truth label (one-hot encoded), and $`\\hat{y}_i`$ is the predicted score for class `i`.\n",
    "\n",
    "\n",
    "```python\n",
    "torch.nn.CrossEntropyLoss(weight=None, ignore_index=-100,  reduction='mean')\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "* `weight` (Tensor, optional): A manual rescaling weight given to each class. If given, has to be a Tensor of size `C`.\n",
    "* `ignore_index` (int, optional): Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets.\n",
    "* `reduction` (str): Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.\n",
    "\n",
    "\n",
    "\n",
    "`x` is a tensor of shape `(N, C)` where `N` is the batch size and `C` is the number of classes. `y` is a tensor of shape `(N)` with class indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets take a example**\n",
    "\n",
    "We have prediction of Batch size of 5 and 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3607, -0.2859, -0.3938],\n",
       "         [ 0.2429, -1.3833, -2.3134],\n",
       "         [-0.3172, -0.8660,  1.7482],\n",
       "         [-0.2759, -0.9755,  0.4790],\n",
       "         [-2.3652, -0.8047,  0.6587]]),\n",
       " tensor([0, 2, 1, 2, 0]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(seed = 100)\n",
    "x = torch.randn(size=(5, 3))  \n",
    "y = torch.tensor([0, 2, 1, 2, 0])   \n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss with no reduction:\n",
      " tensor([0.6902, 2.7987, 2.7966, 0.5327, 3.2708])\n"
     ]
    }
   ],
   "source": [
    "cross_entropy_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "loss = cross_entropy_loss(x, y)\n",
    "print(\"Cross Entropy Loss with no reduction:\\n\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets confirm it through formula :\n",
    "For the `sample : [ 0.3607, -0.2859, -0.3938]` and the `actual class is 0`\n",
    " ```math\n",
    "\\text{Cross Entropy Loss} = -\\sum_{i=1}^{C} y_i \\log\\left(\\frac{\\exp(x_i)}{\\sum_{j=1}^{C} \\exp(x_j)}\\right) \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this formula the inner part inside the bracket applies the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69018966"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_np = x.numpy()\n",
    "y_np = y.numpy()\n",
    "exp_x = np.exp(x_np[0]) # Calulate the exponential of the first sample data\n",
    "calc_loss = - np.log(exp_x[0] / np.sum(exp_x)) # Take data of index 0 as the the actual class for this sample is 0.\n",
    "calc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $`y_i`$ value becomes 1 for index i = 0 and for all other case it is zero. So we calculated loss using only index 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss with mean reduction:\n",
      " tensor(2.0178)\n"
     ]
    }
   ],
   "source": [
    "mean_loss = nn.CrossEntropyLoss(reduction='mean')(x, y)\n",
    "print(\"Cross Entropy Loss with mean reduction:\\n\", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss with sum reduction:\n",
      " tensor(10.0890)\n"
     ]
    }
   ],
   "source": [
    "sum_loss = nn.CrossEntropyLoss(reduction='sum')(x, y)\n",
    "print(\"Cross Entropy Loss with sum reduction:\\n\", sum_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTCLoss\n",
    "\n",
    "This is used in cases where have to map a sequence X to sequence Y. For example these are manily used in speech recognition.\n",
    "\n",
    "It stands for Connectionist Temporal Classification Loss.  It calculates loss between a conitnuos time series and target sequence.\n",
    "\n",
    "\n",
    "```python\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input,target, input_lengths, target_lengths)\n",
    "```\n",
    "**Parameters**\n",
    "\n",
    "* reduction : Specifies the reduction to apply to the output `none` | `mean` | `sum`\n",
    "* zero_infinity (bool) : whether to zero if any infinite losses and the associated gradients occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTC is basically an algorithm employed to train deep neural networks in task such as speech recognition and handwriting recognition or other sequential problems where there is no information about alignment between input and output.\n",
    "\n",
    "**`CTC Model`** \n",
    "\n",
    "In speech recognition, we have an audio clip and its transcribed word as output. But audio clip and transcibed word can be different for different people. Same word can be said in different way meaning person can emphasize different part of word when speaking. So , for large dataset, hand labeling individual data would be impractical.\n",
    "For this type of tasks we use CTC Loss.\n",
    "\n",
    "Applications of CTC\n",
    "\n",
    "The CTC algorithm finds application in domains which do not require explicit alignment information between inputs and outputs during training like\n",
    "\n",
    "    Speech Recognition\n",
    "    Music Transcription\n",
    "    Gesture Recognition\n",
    "    In processing sensor data for robotics system\n",
    "\n",
    "\n",
    "```python\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "```\n",
    "\n",
    "The arguments that needs to be passed are\n",
    "\n",
    "    log_probs: The input sequence of log probabilities. It is typically the output of a neural network applied to the input sequence.\n",
    "    targets: The target sequence. This is usually a 1-dimensional tensor of class indices.\n",
    "    input_lengths: A 1-dimensional tensor containing the lengths of each sequence in the batch.\n",
    "    target_lengths: A 1-dimensional tensor containing the lengths of each target sequence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ekbana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
