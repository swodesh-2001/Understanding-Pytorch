{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tensors on GPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use pytorch with GPU we need cuda support.\n",
    "\n",
    "we can check the cuda support through\n",
    "\n",
    "```bash\n",
    "nvidia-smi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nvidia-smi](./image/nvidia-smi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking pytorch for cuda support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can automatically setup our code to run on either gpu if available as :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Set device type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting number of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Tensors (and models) on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create tensor (default on CPU)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], device='cuda:0') cuda:0\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([1, 2, 3])\n",
    "print(tensor, tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moving the tensors to GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it will be important to convert the tensors between the devices. Tensors on one device wont perform operation with tensors on another device. Lets look at some condition where having tensors on GPU can cause errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tensor_on_gpu.numpy()\n",
    "```\n",
    "\n",
    "This code throws error, because Numpy isn't setup for utilizing GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we convert the tensor back to CPU then utilize the `.numpy()` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tensor_on_cpu = tensor_on_gpu.cpu()\n",
    "tensor_on_cpu.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the default device globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Processing with Python\n",
    "\n",
    "To perform parallel processing we distribute computation tasks across multiple GPUs or computations within a single GPU.\n",
    "\n",
    "`torch.nn.DataParallel` module simplifies parallel processing across multiple GPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lets see the time difference between CPU and GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU_time =  0.011161327362060547\n",
      "GPU_time =  0.0002484321594238281\n"
     ]
    }
   ],
   "source": [
    "#Initialisation of tensors\n",
    "dim=2000\n",
    "\n",
    "start_time = time.time()\n",
    "x = torch.randn(dim,dim).to('cpu') \n",
    "elapsed_time = time.time() - start_time\n",
    "print('CPU_time = ',elapsed_time)\n",
    "\n",
    "start_time = time.time()\n",
    "x=torch.randn((dim,dim), device=torch.device(\"cuda:0\"))\n",
    "elapsed_time = time.time() - start_time\n",
    "print('GPU_time = ',elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Training with GPU Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5157],\n",
       "        [0.5141],\n",
       "        [0.3848],\n",
       "        [0.5058],\n",
       "        [0.4372],\n",
       "        [0.3297],\n",
       "        [0.3459],\n",
       "        [0.2833],\n",
       "        [0.3914],\n",
       "        [0.3355],\n",
       "        [0.4457],\n",
       "        [0.3978],\n",
       "        [0.4221],\n",
       "        [0.4410],\n",
       "        [0.3317],\n",
       "        [0.3787]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example model\n",
    "class Generate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generate, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(5,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "model = Generate() # Initialize the model\n",
    "model.to('cuda') # Move the model to the GPU\n",
    "\n",
    "# Create input data inside GPU\n",
    "input_data = torch.randn(16, 5, device=device)\n",
    "output = model(input_data) # Forward pass on theGP\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supplies the `torch.cuda.empty_cache()` function, which aids in releasing GPU memory that is no longer in use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model ensembling refers to  combining decisions from multiple models to improve the overall performance.   \n",
    "The combining decision could be through voting, averaging or other statistical techniques. Some of the popular methods are :\n",
    "* Bagging - Trains multiple independent model and combines their output.  \n",
    "* Boosting - Trains multiple model where each new model tries to rectify the mistakes of previous model.\n",
    "* Stacking - Trains multiple models and the output of these models are used to train a meta-learner which combines the output to generate a single prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally model ensembling is done by running each model on some inputs seperately and then combining the predictions.\n",
    "\n",
    "Another approach is to combine different model with same architecture to combine them together using torch.vmap.\n",
    "\n",
    "    `Vmap` helps in vectorization and reduces `for` loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Create a simple `Multi Layer Perceptron`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(-2)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform a simple forward pass on this model using a dummy 28 * 28 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669 µs ± 16.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model1 = SimpleMLP()\n",
    "random_image = torch.randn(28,28)\n",
    "output = model1(random_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets create batch of dummy data and make prediction from different models.**  \n",
    "Here we have a 64 image data in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "num_models = 10\n",
    "\n",
    "data = torch.randn(100, 64 , 28, 28, device=device)\n",
    "targets = torch.randint(10, (6400,), device=device)\n",
    "\n",
    "models = [SimpleMLP().to(device) for _ in range(num_models)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created 10 models and kept it in GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a random data batch and make prediction from the 10 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_data_index = random.randint(0,99)\n",
    "random_data_batch =  data[random_data_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.72 ms ± 37.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "predictions = [model(random_data_batch) for model in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of using for loop to make prediciton we will be using Vmap to vectorize this predictions.\n",
    "\n",
    "Each model is composed of different layers. Now if their architecture is same we can stack these layers together. Now to perform this we use \n",
    "`stack_module_state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import stack_module_state\n",
    "params , buffers = stack_module_state(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Vmap these models we will create a funciton , that receives these parameters , buffers and inputs. Now from these parameters it produces output.\n",
    "And that funciton is `torch.func.functional_call`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.func.functional_call`\n",
    "\n",
    "Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets look at the example below to understand this , instead of having a module with its fixed parameter , we can make the module act as functional where we can pass its parameters to computer the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a simple linear layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "output_dim = 3\n",
    "linear_layer = nn.Linear(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating some example input data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.randn(1, input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting the parameters of the linear layer which we will pass it to the functional call**.  \n",
    "The parameters must be in dictionary format as below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {parameter_name: param for parameter_name, param in linear_layer.named_parameters()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use `torch.func.functional_call` to apply the linear layer with the extracted parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output data: tensor([[-0.2488,  0.4308, -0.0601]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.func import functional_call\n",
    "output = functional_call(linear_layer, parameters, input_data)\n",
    "\n",
    "print(\"Output data:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus using functional_call we made nn.Linear act as a function where we can change the module parameters easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply this to make prediction with the 10 models which we created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are construct a \"stateless\" version of one of the models. It is \"stateless\" in the sense that the parameters are meta Tensors and do not have storage.  \n",
    "Meta tensor are tensors without contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call\n",
    "import copy\n",
    "\n",
    "base_model = copy.deepcopy(models[0])\n",
    "base_model = base_model.to('meta') \n",
    "\n",
    "def fmodel(params, buffers, x):\n",
    "    return functional_call(base_model, (params, buffers), (x,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get the prediciton as we did previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675 µs ± 44.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "from torch import  vmap\n",
    "predictions_vmap = vmap(fmodel, in_dims=(0, 0, None))(params, buffers, random_data_batch) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we got a huge performance boost by using vmap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ekbana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
